import pandas as pd
import numpy as np
import json
import warnings

warnings.filterwarnings('ignore')


class EChartsOptimizedDataPreparator:
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®å‡†å¤‡å™¨"""
        self.df_long = None
        self.df_wide = None
        self.load_clean_data()

    def load_clean_data(self):
        """åŠ è½½é¢„å¤„ç†åçš„æ¸…æ´æ•°æ®"""
        try:
            print(" åŠ è½½æ¸…æ´æ•°æ®...")
            self.df_long = pd.read_csv('æ¸…æ´æ•°æ®_é•¿æ ¼å¼.csv')
            self.df_wide = pd.read_csv('æ¸…æ´æ•°æ®_å®½æ ¼å¼.csv')
            print(f" æ•°æ®åŠ è½½æˆåŠŸ: {self.df_long.shape[0]} è¡Œ, {len(self.df_long['Country'].unique())} ä¸ªå›½å®¶")
        except Exception as e:
            print(f" æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return

    def prepare_sankey_data(self):
        print("\nğŸŒŠ å‡†å¤‡æ¡‘åŸºå›¾æ•°æ®...")

        # ä½¿ç”¨2020å¹´æ•°æ®åˆ›å»ºåŒºåŸŸåˆ°å›½å®¶çš„æ•°æ®æµ
        sankey_data = self.df_long[self.df_long['Year'] == 2020].copy()

        # åŒºåŸŸæ˜ å°„
        region_mapping = {
            'United States of America': 'North America',
            'Canada': 'North America',
            'European Union (Convention)': 'Europe',
            'Germany': 'Europe',
            'United Kingdom of Great Britain and Northern Ireland': 'Europe',
            'France': 'Europe',
            'Italy': 'Europe',
            'Spain': 'Europe',
            'Netherlands': 'Europe',
            'Poland': 'Europe',
            'Russian Federation': 'Asia',
            'Japan': 'Asia',
            'Kazakhstan': 'Asia',
            'TÃ¼rkiye': 'Asia',
            'Australia': 'Oceania',
            'New Zealand': 'Oceania'
        }

        sankey_data['Region'] = sankey_data['Country'].map(region_mapping).fillna('Other')

        # é€‰æ‹©å‰12ä¸ªæ’æ”¾å¤§å›½
        top_countries = sankey_data.nlargest(12, 'Emissions')

        # æ„å»ºæ¡‘åŸºå›¾æ•°æ®ç»“æ„
        nodes = []
        links = []

        # æ·»åŠ åŒºåŸŸèŠ‚ç‚¹
        regions = top_countries['Region'].unique()
        for region in regions:
            nodes.append({'name': str(region), 'category': 'region'})  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²

        # æ·»åŠ å›½å®¶èŠ‚ç‚¹
        for _, row in top_countries.iterrows():
            nodes.append({'name': str(row['Country']), 'category': 'country'})  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²

        # æ·»åŠ å…¨çƒæ€»èŠ‚ç‚¹
        nodes.append({'name': 'å…¨çƒæ’æ”¾æ€»é‡', 'category': 'global'})

        # åˆ›å»ºè¿æ¥ï¼šåŒºåŸŸåˆ°å›½å®¶
        for _, row in top_countries.iterrows():
            links.append({
                'source': str(row['Region']),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                'target': str(row['Country']),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                'value': float(row['Emissions'])  # è½¬æ¢ä¸ºPython float
            })

        # åˆ›å»ºè¿æ¥ï¼šå›½å®¶åˆ°å…¨çƒ
        for _, row in top_countries.iterrows():
            links.append({
                'source': str(row['Country']),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                'target': 'å…¨çƒæ’æ”¾æ€»é‡',
                'value': float(row['Emissions'])  # è½¬æ¢ä¸ºPython float
            })

        sankey_result = {
            'nodes': nodes,
            'links': links,
            'title': '2020å¹´å…¨çƒç¢³æ’æ”¾æµå‘å›¾'
        }

        # ä¿å­˜æ•°æ®
        with open('ECharts_æ¡‘åŸºå›¾æ•°æ®.json', 'w', encoding='utf-8') as f:
            json.dump(sankey_result, f, ensure_ascii=False, indent=2)

        print(" æ¡‘åŸºå›¾æ•°æ®å·²ä¿å­˜: ECharts_æ¡‘åŸºå›¾æ•°æ®.json")
        return sankey_result

    def prepare_radar_data(self):
        print("\n å‡†å¤‡é›·è¾¾å›¾æ•°æ®...")

        # é€‰æ‹©å‰6ä¸ªæ’æ”¾å¤§å›½è¿›è¡Œå¤šç»´åº¦æ¯”è¾ƒ
        top_countries = self.df_long.groupby('Country')['Emissions'].sum().nlargest(6).index

        # è®¡ç®—å„ä¸ªç»´åº¦çš„æŒ‡æ ‡
        radar_data = {}

        for country in top_countries:
            country_data = self.df_long[self.df_long['Country'] == country]

            # è®¡ç®—å„ç»´åº¦æŒ‡æ ‡
            total_emissions = country_data['Emissions'].sum()
            avg_emissions = country_data['Emissions'].mean()
            max_emissions = country_data['Emissions'].max()

            # è®¡ç®—å¢é•¿ç‡
            first_year = country_data[country_data['Year'] == country_data['Year'].min()]['Emissions'].iloc[0]
            last_year = country_data[country_data['Year'] == country_data['Year'].max()]['Emissions'].iloc[0]
            growth_rate = ((last_year - first_year) / first_year * 100) if first_year > 0 else 0

            # è®¡ç®—æ³¢åŠ¨æ€§ï¼ˆæ ‡å‡†å·®ï¼‰
            volatility = country_data['Emissions'].std()

            # è®¡ç®—è¿‘æœŸè¶‹åŠ¿ï¼ˆæœ€è¿‘10å¹´å¹³å‡ï¼‰
            recent_data = country_data[country_data['Year'] >= 2010]
            recent_avg = recent_data['Emissions'].mean()

            radar_data[country] = {
                'total_emissions': total_emissions,
                'avg_emissions': avg_emissions,
                'max_emissions': max_emissions,
                'growth_rate': growth_rate,
                'volatility': volatility,
                'recent_avg': recent_avg
            }

        # è®¾ç½®å›ºå®šçš„æœ€å¤§å€¼èŒƒå›´ï¼Œç¡®ä¿é›·è¾¾å›¾æœ‰åŒºåˆ†åº¦
        all_values = list(radar_data.values())

        # è®¡ç®—åˆé€‚çš„æœ€å¤§å€¼ï¼Œå¢åŠ åŒºåˆ†åº¦
        max_total = max([d['total_emissions'] for d in all_values])
        max_avg = max([d['avg_emissions'] for d in all_values])
        max_peak = max([d['max_emissions'] for d in all_values])
        max_growth = max([abs(d['growth_rate']) for d in all_values]) + 50  # å¢åŠ ä¸€äº›èŒƒå›´
        max_volatility = max([d['volatility'] for d in all_values])
        max_recent = max([d['recent_avg'] for d in all_values])

        indicators = [
            {'name': 'æ€»æ’æ”¾é‡', 'max': float(max_total * 1.2)},  # å¢åŠ 20%ä½™é‡
            {'name': 'å¹³å‡æ’æ”¾é‡', 'max': float(max_avg * 1.2)},
            {'name': 'å³°å€¼æ’æ”¾é‡', 'max': float(max_peak * 1.2)},
            {'name': 'å¢é•¿ç‡', 'max': 100.0},  # å›ºå®šæœ€å¤§å€¼
            {'name': 'æ³¢åŠ¨æ€§', 'max': float(max_volatility * 1.5)},  # å¢åŠ æ›´å¤šä½™é‡
            {'name': 'è¿‘æœŸå¹³å‡', 'max': float(max_recent * 1.2)}
        ]

        # æ„å»ºé›·è¾¾å›¾æ•°æ® - ä½¿ç”¨åŸå§‹æ•°å€¼è€Œéç™¾åˆ†æ¯”
        radar_series = []
        for country, values in radar_data.items():
            radar_values = [
                float(values['total_emissions']),
                float(values['avg_emissions']),
                float(values['max_emissions']),
                float(abs(values['growth_rate'])),
                float(values['volatility']),
                float(values['recent_avg'])
            ]

            radar_series.append({
                'name': str(country),
                'value': radar_values
            })

        radar_result = {
            'indicators': indicators,
            'series': radar_series,
            'title': 'ä¸»è¦æ’æ”¾å›½å¤šç»´åº¦é›·è¾¾å›¾'
        }

        # ä¿å­˜æ•°æ®
        with open('ECharts_é›·è¾¾å›¾æ•°æ®.json', 'w', encoding='utf-8') as f:
            json.dump(radar_result, f, ensure_ascii=False, indent=2)

        print(" é›·è¾¾å›¾æ•°æ®å·²ä¿å­˜: ECharts_é›·è¾¾å›¾æ•°æ®.json")
        return radar_result

    def prepare_animated_line_data(self):
        print("\n å‡†å¤‡åŠ¨ç”»æ—¶é—´åºåˆ—æ•°æ®...")

        # é€‰æ‹©å‰8ä¸ªæ’æ”¾å¤§å›½
        top_countries = self.df_long.groupby('Country')['Emissions'].sum().nlargest(8).index
        line_data = self.df_long[self.df_long['Country'].isin(top_countries)].copy()

        # æŒ‰å¹´ä»½å’Œå›½å®¶ç»„ç»‡æ•°æ®
        years = sorted(line_data['Year'].unique())

        # æ„å»ºEChartséœ€è¦çš„æ—¶é—´åºåˆ—æ•°æ®æ ¼å¼
        series_data = []

        for country in top_countries:
            country_data = line_data[line_data['Country'] == country].sort_values('Year')
            country_series = {
                'name': str(country),
                'type': 'line',
                'data': []
            }

            for year in years:
                year_data = country_data[country_data['Year'] == year]
                if not year_data.empty:
                    country_series['data'].append(float(year_data['Emissions'].iloc[0]))
                else:
                    country_series['data'].append(0)

            series_data.append(country_series)

        # æ„å»ºæ—¶é—´åºåˆ—æ•°æ®
        timeseries_data = {
            'years': [int(year) for year in years],  # xè½´æ•°æ®
            'series': series_data,  # ç³»åˆ—æ•°æ®
            'title': 'ä¸»è¦æ’æ”¾å›½æ—¶é—´è¶‹åŠ¿åŠ¨ç”»'
        }

        # ä¿å­˜æ•°æ®
        with open('ECharts_åŠ¨ç”»æ—¶é—´åºåˆ—æ•°æ®.json', 'w', encoding='utf-8') as f:
            json.dump(timeseries_data, f, ensure_ascii=False, indent=2)

        print(" åŠ¨ç”»æ—¶é—´åºåˆ—æ•°æ®å·²ä¿å­˜: ECharts_åŠ¨ç”»æ—¶é—´åºåˆ—æ•°æ®.json")
        return timeseries_data

    def prepare_3d_surface_data(self):
        print("\n å‡†å¤‡3Dè¡¨é¢å›¾æ•°æ®...")

        # é€‰æ‹©å‰10ä¸ªå›½å®¶åˆ›å»º3Dæ•°æ®
        top_countries = self.df_long.groupby('Country')['Emissions'].sum().nlargest(10).index
        surface_data = self.df_long[self.df_long['Country'].isin(top_countries)].copy()

        # åˆ›å»ºå›½å®¶-å¹´ä»½çŸ©é˜µ
        pivot_data = surface_data.pivot(index='Country', columns='Year', values='Emissions')
        pivot_data = pivot_data.fillna(0)

        # æ„å»º3Dè¡¨é¢æ•°æ®
        countries_list = list(pivot_data.index)
        years_list = list(pivot_data.columns)

        # å‡†å¤‡æ•°æ®ç‚¹
        data_points = []
        for i, country in enumerate(countries_list):
            for j, year in enumerate(years_list):
                value = pivot_data.loc[country, year]
                data_points.append([i, j, float(value)])  # è½¬æ¢ä¸ºPython float

        # æ„å»ºåæ ‡è½´æ•°æ®
        surface_result = {
            'xAxis': {
                'name': 'å›½å®¶',
                'data': [str(country) for country in countries_list]  # ç¡®ä¿ä¸ºå­—ç¬¦ä¸²
            },
            'yAxis': {
                'name': 'å¹´ä»½',
                'data': [int(year) for year in years_list]  # è½¬æ¢ä¸ºPython int
            },
            'zAxis': {
                'name': 'æ’æ”¾é‡ (kt CO2å½“é‡)'
            },
            'data': data_points,
            'title': 'ä¸»è¦å›½å®¶æ’æ”¾é‡3Dè¡¨é¢å›¾'
        }

        # æ·»åŠ é¢å¤–çš„3Dæ•£ç‚¹æ•°æ®ç”¨äºå¯¹æ¯”
        scatter_3d_data = []
        for _, row in surface_data.iterrows():
            country_index = countries_list.index(row['Country'])
            year_index = years_list.index(row['Year'])
            scatter_3d_data.append({
                'country': str(row['Country']),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                'year': int(row['Year']),  # è½¬æ¢ä¸ºPython int
                'emissions': float(row['Emissions']),  # è½¬æ¢ä¸ºPython float
                'coordinates': [country_index, year_index, float(row['Emissions'])]  # è½¬æ¢ä¸ºPython float
            })

        surface_result['scatter_data'] = scatter_3d_data

        # ä¿å­˜æ•°æ®
        with open('ECharts_3Dè¡¨é¢å›¾æ•°æ®.json', 'w', encoding='utf-8') as f:
            json.dump(surface_result, f, ensure_ascii=False, indent=2)

        print(" 3Dè¡¨é¢å›¾æ•°æ®å·²ä¿å­˜: ECharts_3Dè¡¨é¢å›¾æ•°æ®.json")
        return surface_result


def main():
    """ä¸»å‡½æ•°"""
    print(" EChartsä¼˜åŒ–æ•°æ®å‡†å¤‡")
    print("=" * 60)

    preparator = EChartsOptimizedDataPreparator()

    if preparator.df_long is None:
        return

    try:
        # å‡†å¤‡4ä¸ªæœ€é€‚åˆEChartsçš„å›¾è¡¨æ•°æ®
        preparator.prepare_sankey_data()
        preparator.prepare_radar_data()
        preparator.prepare_animated_line_data()
        preparator.prepare_3d_surface_data()

        print(f"\n EChartsæ•°æ®å‡†å¤‡å®Œæˆï¼")
        print(" ç”Ÿæˆçš„æ–‡ä»¶:")
        print("    ECharts_æ¡‘åŸºå›¾æ•°æ®.json")
        print("    ECharts_é›·è¾¾å›¾æ•°æ®.json")
        print("    ECharts_åŠ¨ç”»æ—¶é—´åºåˆ—æ•°æ®.json")
        print("    ECharts_3Dè¡¨é¢å›¾æ•°æ®.json")
        print("    EChartsæ•°æ®è¯´æ˜.md")
        print("\n è¿™4ä¸ªå›¾è¡¨ç±»å‹å……åˆ†å‘æŒ¥äº†EChartsçš„ä¼˜åŠ¿ï¼")

    except Exception as e:
        print(f" æ•°æ®å‡†å¤‡è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()